{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c083cdd4",
   "metadata": {},
   "source": [
    "# Lego Ebay Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4427e4-5b86-4acb-ab51-a16a949df422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zubaz\\anaconda3\\envs\\Lego Data Analysis\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n",
      "  from . import _distributor_init\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"C:\\Users\\zubaz\\anaconda3\\envs\\Lego Data Analysis\\python.exe\"\n  * The NumPy version is: \"1.21.2\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18796/2232570428.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msql\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Lego Data Analysis\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;34m\"Unable to import required dependencies:\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.8 from \"C:\\Users\\zubaz\\anaconda3\\envs\\Lego Data Analysis\\python.exe\"\n  * The NumPy version is: \"1.21.2\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3 as sql\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import scipy.stats as stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10311682-291a-4e05-90be-cb6a5fd8a343",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **********   INJEST   **********\n",
    "Create a connection to the database and save SQL queries as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed600996-7d01-427e-acc2-4e13e739343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection to database\n",
    "database = \"C:\\\\Users\\\\zubaz\\\\Documents\\\\Python\\\\EbayLegoWebscrape\\\\lego.db\"\n",
    "connection = sql.connect(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa11ded-1896-460a-a34b-c09b0eb5b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 queries for 2 tables, one with price data, one with set metadata\n",
    "query1 = '''SELECT item_num, set_num, date, price\n",
    "            FROM ebay_prices'''\n",
    "query2 = '''SELECT * FROM set_details'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6991f4-65d8-45f9-a28a-1bb34608324b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create dataframe for query1\n",
    "This dataframe contains the ebay prices data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170d110-226f-4440-b214-f4a9b5565fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(query1, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee50c08-2f7b-4705-9705-4d78ce4f4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218057e-b1ea-4ab1-9419-6a6c41376ba7",
   "metadata": {},
   "source": [
    "### Create a dataframe for query2\n",
    "This dataframe contains the set dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2256680-7b74-4035-9782-c89be319e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set = pd.read_sql_query(query2, connection)\n",
    "df_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87a62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close connection to database\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1e16c",
   "metadata": {},
   "source": [
    "Get some basic date and set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3459308-d765-4b91-9a00-53b906d4aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change date column from text to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59636e-753a-4913-8917-1256d236da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find earliest and latest dates and how many days of data exists\n",
    "oldest_date = df['date'].min()\n",
    "recent_date = df['date'].max()\n",
    "date_difference = recent_date - oldest_date\n",
    "num_of_rows = len(df.index)\n",
    "num_of_sets = len(pd.unique(df['set_num']))\n",
    "num_of_items = len(pd.unique(df['item_num']))\n",
    "\n",
    "print(f'Earliest price data: {oldest_date}')\n",
    "print(f'Latest price data: {recent_date}')\n",
    "print(f'Days of data: {date_difference}')\n",
    "print(f\"Price data for {num_of_sets} Lego sets\")\n",
    "print(f'Total rows: {num_of_rows}')\n",
    "print(f\"Total unique listings: {num_of_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff340f4-4bbd-47f7-8755-f00f5cd3c0c5",
   "metadata": {},
   "source": [
    "# **********  EXPLORE & CLEAN  **********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0113b",
   "metadata": {},
   "source": [
    "## Price Dataframe\n",
    "Examine the 'price' column and do some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010b99b-053a-4c70-9f34-6b2ee11d6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values in price column are integers\n",
    "all(x.is_integer() for x in df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69cd7a-5186-46ee-8dc0-aa8d18a01e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[pd.to_numeric(df['price'], errors='coerce').isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bb4b5-fb7f-44b9-a996-7309b1bda54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like the commas are bad, let's replace commas with nothing\n",
    "df['price'] = df['price'].replace(',','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69ed67-f24f-49aa-8fa6-2a5155b5b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see what the rest of the non numeric values look like\n",
    "# print(df['price'] [pd.to_numeric(df['price'], errors='coerce').isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09393d11-d9d1-44a9-bc10-f78afb86649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10819568-01a3-468c-9702-6eac25573953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the index of rows with 'to' in them\n",
    "# these values are too hard to deal with, probably not representative listings\n",
    "remove_rows = df[df['price'].str.contains(\"to\") == True].index\n",
    "\n",
    "print(remove_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a5366-8a3e-424a-b481-13368ed98043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove these rows\n",
    "df.drop(remove_rows, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8b64a-89e3-4686-b9fc-095073ffa060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[pd.to_numeric(df['price'], errors='coerce').isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a problem with my scraping, it's collecting GBP and EUR values\n",
    "# I will need to check on this, for now just filter non-digit rows out\n",
    "df = df[pd.to_numeric(df['price'], errors='coerce').notnull()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814797ca-ff03-44b7-869a-382b99a29134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks price column to make sure all rows are numeric\n",
    "pd.to_numeric(df['price'], errors='coerce').notnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ec799-2cdf-48a4-9bb4-6a1a21add39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a6eb7-bdff-4083-8dd8-261f5e933d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that all rows in price column are numeric, change column data type to numeric\n",
    "# pandas will pick int64 if there are no decimals, float 64 if decimals are present, which there are\n",
    "df['price'] = pd.to_numeric(df['price'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a25cd",
   "metadata": {},
   "source": [
    "## Metadata Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530b436-1575-4881-a8d9-f3d3196ffb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean the set_details data\n",
    "df_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37950b96-21a1-44b8-a8d2-68d4cd479fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove -1 from set_num\n",
    "df_set['set_num'] = df_set['set_num'].str.split('-', n=1).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b346fec-e209-423b-b716-23334fba7494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split launch_exit column into 2 columns\n",
    "df_set[['launch_date', 'retirement_date']] = df_set['launch_exit'].str.split(' - ', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356c47a-9fb8-43a6-a4d7-37a7c0c9e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.drop('launch_exit', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09bb60-6f59-405f-8838-1c3cb6a25aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to plit the minifigs column into total and unique\n",
    "df_set[['minifigs_total', 'minifigs_unique']] = df_set['minifigs'].str.split(' ', n=1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f74f95-c51c-4bc0-b5d6-0ac218f39ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the minifig value from minifigs_unique\n",
    "df_set['minifigs_unique'] = df_set['minifigs_unique'].str.split(' ', n=1).str[0].str.replace('(', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d7413-e9e9-49b7-8858-8264a8ac67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.drop('minifigs', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f04852-b4fd-4462-a137-9cd950211e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse rating column to get the text after the stars, this grabs non numeric values for rows with no rating\n",
    "df_set['rating'] = df_set['rating'].str.split(' ', n=2).str[1].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d7887-213b-46a6-9194-f1cb1a0260fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a good way to replace non-numeric rows is to_numeric method, \n",
    "# must use coerce to force NaN values for non-numerics\n",
    "df_set['rating'] = pd.to_numeric(df_set['rating'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e92b2-8d56-4d05-8890-4013773ba544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change launch retirement date columns to date\n",
    "# first need to remove spaces\n",
    "df_set['launch_date'] = df_set['launch_date'].str.replace(' ', '')\n",
    "df_set['launch_date'] = pd.to_datetime(df_set['launch_date'], format='%d%b%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588760cb-948a-4d4d-9de8-d561a21f0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# had some t.b.a text for some rows, they threw an error, added errors=coerce and seemed to fix it\n",
    "df_set['retirement_date'] = df_set['retirement_date'].str.replace(' ', '')\n",
    "df_set['retirement_date'] = pd.to_datetime(df_set['retirement_date'], format='%d%b%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20e8d2-912d-4b72-b64f-e54cc2e8796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up msrp column and grab USD values only - drop pounds and euro values if present\n",
    "# use regex to extract everything after the $, the dot, and the remaining digits\n",
    "# REGEX sucks. But remember to use regex101.com, it's a life saver\n",
    "df_set['msrp'] = df_set['msrp'].str.extract(r\"\\$(\\d+\\.\\d+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08311c27-9f31-41e7-a52a-a8dad9348219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if launch_date is empty add the value from year released date column, \n",
    "df_set['launch_date'] = df_set['launch_date'].fillna(df_set['year_released'])\n",
    "# very cool, it added jan 1 to the year automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2664d39-5eb0-4f01-bea6-dfdbde50b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change some datatypes\n",
    "df_set['year_released'] = df_set['year_released'].astype(int)\n",
    "df_set['msrp'] = df_set['msrp'].astype(float)\n",
    "df_set['minifigs_total'] = df_set['minifigs_total'].astype(float)\n",
    "df_set['minifigs_unique'] = df_set['minifigs_unique'].astype(float)\n",
    "df_set['set_num'] = df_set['set_num'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a66251-5d24-4032-9e2d-fa30d9a5181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1644318",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68dfe6-eb7c-4e4e-9970-93de39e99c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index to set_num, which should be unique\n",
    "df_set.set_index('set_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ddc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for all unique rows in set_num\n",
    "# is_unique method only works for a series, create that first and then check for uniqueness\n",
    "set_num_series = df_set['set_num'].squeeze()\n",
    "set_num_series.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final form of the set_details data\n",
    "# Therefore we can create a table in db for this which will be the dimension table in pbi\n",
    "\n",
    "database = \"C:\\\\Users\\\\zubaz\\\\Documents\\\\Python\\\\EbayLegoWebscrape\\\\lego.db\"\n",
    "connection = sql.connect(database)\n",
    "\n",
    "df_set.to_sql('set_details_cleaned', connection, if_exists='replace', index=False)\n",
    "# if_exists=replace I don't know what has and hasn't changed, nulls are difficult to update\n",
    "# this is the easiest way to update the whole table.\n",
    "# But it does mean I will need to scrape ALL the set nums from brickset to capture all the changes\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259220b5",
   "metadata": {},
   "source": [
    "Loop through a list of sets and calculate and remove outliers, then groupby and average daily price\n",
    "Create a new data frame that we will use for the rest of the analysis that is now at a daily granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61505c-6d3f-48a4-a510-a5c1b2969816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to remove outliers using IQR method\n",
    "def remove_outliers(dataframe):\n",
    "    \"\"\"\n",
    "    Function to identify and remove outliers from the price column using IQR method\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataframe : a dataframe\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dataframe with outliers removed\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Q1 = dataframe['price'].quantile(0.25)\n",
    "        Q3 = dataframe['price'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_lim = Q1 # the 1.5*IQR wasn't catching enough low prices\n",
    "        upper_lim = Q3 + 1.0*IQR # 1.5*IQR was too high for most sets\n",
    "        outliers_15_low = (dataframe['price'] < lower_lim)\n",
    "        outliers_15_high = (dataframe['price'] > upper_lim)\n",
    "        df_outliers_removed = dataframe[~(outliers_15_low | outliers_15_high)]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return df_outliers_removed\n",
    "    \n",
    "    # I could add another parameter for column name to make this more generic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list\n",
    "# I can't figure out how to import search_sets.py from another folder.\n",
    "# So I'll just recreate it here\n",
    "\n",
    "# I copied this function from search_sets.py\n",
    "# I should try this one day to help with importing functions from another folder\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\nn_webserver\")\n",
    "\n",
    "from employee import motivation_to_work\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_search_list():\n",
    "\n",
    "    import pandas as pd\n",
    "    import sqlite3\n",
    "\n",
    "    # create a connection to the database\n",
    "    database = \"C:\\\\Users\\\\zubaz\\\\Documents\\\\Python\\\\EbayLegoWebscrape\\\\lego.db\"\n",
    "    connection = sql.connect(database)\n",
    "\n",
    "    query1 = ''' SELECT DISTINCT SetNum\n",
    "            FROM brickset_set_nums\n",
    "            WHERE NumPieces > 0\n",
    "    '''\n",
    "\n",
    "    df = pd.read_sql_query(query1, connection)\n",
    "\n",
    "    connection.close()\n",
    "\n",
    "    # remove -1 from set_num\n",
    "    df['SetNum'] = df['SetNum'].str.split('-', n=1).str[0]\n",
    "\n",
    "    # create list from df column\n",
    "    search_terms = df['SetNum'].tolist()\n",
    "\n",
    "    # Remove duplicates\n",
    "    search_terms = list(dict.fromkeys(search_terms))\n",
    "    search_terms.sort(reverse=True)\n",
    "    # print(search_terms)\n",
    "    # print(len(search_terms))\n",
    "\n",
    "\n",
    "    return search_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had a major problem with the loop not working, turns out the search_terms \n",
    "# is a list of strings and the df we are looping through has set_num as int\n",
    "# So I cast int to all the value in the for loop below\n",
    "# But this removes the COMCON and other string set numbers, I should change the set_num\n",
    "# back to string and remove it from the index in the above cells.\n",
    "list_of_sets2 = create_search_list()\n",
    "print(len(list_of_sets2))\n",
    "\n",
    "# change all list items to integer\n",
    "new_list = []\n",
    "for value in list_of_sets2:\n",
    "    try:\n",
    "        new_list.append(int(value))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(len(new_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8047e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test list of sets to loop through\n",
    "# test_list = [75827, 75192, 70222, 70223]\n",
    "\n",
    "# create an empty data frame to append each looped df to\n",
    "looped_df = pd.DataFrame()\n",
    "df_no_outliers_pbi = pd.DataFrame()\n",
    "\n",
    "# filter by set num and then perform remove_outliers\n",
    "for set in new_list:\n",
    "    filter = (df['set_num'] == set)\n",
    "    df_filtered = df[filter]\n",
    "    df_no_outliers = remove_outliers(df_filtered)\n",
    "\n",
    "    # create a dataframe for Power BI that has all prices, but no outliers\n",
    "    # perform this BEFORE the groupby so we can do daily analysis later\n",
    "    df_no_outliers_pbi = df_no_outliers_pbi.append(df_no_outliers)\n",
    "    \n",
    "    # calculate ONE average price per day per for each set\n",
    "    # reset index fills down the set_num column, creates a full dataframe\n",
    "    df_filtered_grouped = df_no_outliers.groupby(['set_num', 'date']).mean().reset_index()\n",
    "    looped_df = looped_df.append(df_filtered_grouped)\n",
    "\n",
    "print(f'df shape: {df.shape}')\n",
    "print(f'df_no_ouliers_pbi shape: {df_no_outliers_pbi.shape}')\n",
    "print(f'looped_df shape: {looped_df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set item_num as index to ensure one price per listing\n",
    "df_no_outliers_pbi = df_no_outliers_pbi.set_index('item_num')\n",
    "# df_no_outliers_pbi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df_no_outliers_pbi to sql for import into Power BI\n",
    "\n",
    "database = \"C:\\\\Users\\\\zubaz\\\\Documents\\\\Python\\\\EbayLegoWebscrape\\\\lego.db\"\n",
    "connection = sql.connect(database)\n",
    "\n",
    "df_no_outliers_pbi.to_sql('ebay_no_outliers_pbi', connection, if_exists='replace', index=True)\n",
    "# if_exists=replace this is the simplest method. With append it was fully writing a copy of\n",
    "# the db every time and doubling the rows.\n",
    "# index=True because I set the item_num as the index\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change item_num to int64, for some reason int32 gives scientific notation\n",
    "looped_df['item_num'] = looped_df['item_num'].astype('int64')\n",
    "looped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "looped_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783204a",
   "metadata": {},
   "source": [
    "### Write clean price data to db\n",
    "Write to database a clean dataset of sales data that is loaded into Power BI for analysis.\n",
    "Much easier to do the clean up and outlier detection in python.\n",
    "Also easier to import from SQLite db into Power BI than do the csv dance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1db92",
   "metadata": {},
   "source": [
    "# **********  MERGE  **********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types before the merge\n",
    "df_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7217ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "looped_df['set_num'] = looped_df['set_num'].astype(int)\n",
    "looped_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want all values in the price df and mrsp from set data\n",
    "# this is a left join and we use pandas merge method\n",
    "joined_df = pd.merge(looped_df,\n",
    "                    df_set[['set_num', 'msrp']],\n",
    "                    on = 'set_num',\n",
    "                    how = 'left' \n",
    ")\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18882f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create calculated column of $ of appreciation for every row\n",
    "joined_df['appreciation'] = (joined_df['price'] - joined_df['msrp']).round(2)\n",
    "\n",
    "# create % appreciation calculated column\n",
    "joined_df['pct_appreciation'] = (joined_df['appreciation'] / joined_df['msrp'] * 100).round()\n",
    "\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter all dates to last 30 days from today\n",
    "today = datetime.datetime.now()\n",
    "days30ago = today - pd.Timedelta(days=30)\n",
    "\n",
    "joined_df = joined_df.loc[joined_df.date > days30ago]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate one % appreciation for each set_num\n",
    "group_df = joined_df[['set_num', 'pct_appreciation']]\n",
    "group_df = group_df.groupby('set_num').mean().reset_index()\n",
    "group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fcd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to merge the grouped % appreciation back into the set metadata df\n",
    "final_df = pd.merge(df_set,\n",
    "                    group_df[['set_num', 'pct_appreciation']],\n",
    "                    on = 'set_num',\n",
    "                    how = 'left' \n",
    ")\n",
    "# selected_rows = final_df[~final_df['pct_appreciation'].isnull()]\n",
    "# selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ee959",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv to use in another notebook for playing around with ML\n",
    "final_df.to_csv('final_cleaned_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66659f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
